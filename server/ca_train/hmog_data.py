from __future__ import annotations

import csv
import logging
from dataclasses import dataclass
from pathlib import Path
from typing import Iterator, List, Optional, Sequence, Tuple

import numpy as np
import torch
from torch.utils.data import Dataset

# =============================================================================
# Server window dataset adapter
#
# This module adapts the windowed CSV datasets generated by:
#   /data/code/server/data_storage/processed_data/window/<t>/<user>/{train,val,test}.csv
#
# CSV columns (per-row, per-window):
#   subject, session, timestamp, acc_x..mag_z, window_id
#
# Each window is stored as `window_points` consecutive rows where:
#   window_points = round(window_size_sec * 100Hz)
# and windows do NOT cross sessions.
#
# For the VQ autoencoder we build samples shaped as (1, 12, T):
#   9 raw axes + 3 magnitudes (acc/gyr/mag).
# =============================================================================

logger = logging.getLogger(__name__)

SAMPLING_RATE_HZ = 100
DEFAULT_OVERLAP = 0.5  # the server pipeline uses 50% stride
WINDOW_SIZES = [round(x * 0.1, 1) for x in range(1, 11)]  # 0.1s -> 1.0s

AXIS_COLUMNS = (
    "acc_x",
    "acc_y",
    "acc_z",
    "gyr_x",
    "gyr_y",
    "gyr_z",
    "mag_x",
    "mag_y",
    "mag_z",
)

SENSOR_ORDER = (
    "acc_x",
    "acc_y",
    "acc_z",
    "acc_magnitude",
    "gyr_x",
    "gyr_y",
    "gyr_z",
    "gyr_magnitude",
    "mag_x",
    "mag_y",
    "mag_z",
    "mag_magnitude",
)


def format_window_dir_name(window_size: float) -> str:
    return f"{window_size:.1f}"


def list_available_users(base_path: Path) -> List[str]:
    base_path = Path(base_path)
    user_ids = set()
    for ws in WINDOW_SIZES:
        ws_dir = base_path / format_window_dir_name(ws)
        if not ws_dir.exists():
            continue
        for p in ws_dir.iterdir():
            if p.is_dir():
                user_ids.add(p.name)
    return sorted(user_ids)


class WindowedHMOGDataset(Dataset):
    """Simple in-memory dataset wrapper used by hmog_vq_experiment.py."""

    def __init__(self, windows: np.ndarray, labels: np.ndarray):
        if windows.ndim != 4:
            raise ValueError(f"Expected windows as (N, C, H, W), got shape={windows.shape}")
        if labels.ndim != 1:
            raise ValueError(f"Expected labels as (N,), got shape={labels.shape}")
        if len(windows) != len(labels):
            raise ValueError(f"Mismatched windows/labels: {len(windows)} vs {len(labels)}")
        self.windows = torch.from_numpy(windows).to(dtype=torch.float32, copy=False)
        self.labels = torch.from_numpy(labels).to(dtype=torch.int64, copy=False)

    def __len__(self) -> int:  # noqa: D401 - simple wrapper
        return int(self.labels.shape[0])

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        return self.windows[idx], self.labels[idx]


def _window_points(window_size_sec: float) -> int:
    return max(1, int(round(window_size_sec * SAMPLING_RATE_HZ)))


def _resample_time_axis(window: np.ndarray, target_width: int) -> np.ndarray:
    """Linear-resample a (H, T) window to (H, target_width) along the time axis."""
    if window.shape[1] == target_width:
        return window
    x_old = np.linspace(0.0, 1.0, window.shape[1], dtype=np.float32)
    x_new = np.linspace(0.0, 1.0, target_width, dtype=np.float32)
    out = np.empty((window.shape[0], target_width), dtype=np.float32)
    for i in range(window.shape[0]):
        out[i] = np.interp(x_new, x_old, window[i]).astype(np.float32, copy=False)
    return out


@dataclass
class _SplitLoadSpec:
    max_pos: Optional[int]
    max_neg: Optional[int]
    max_total: Optional[int]
    full_scan: bool
    seed: int


def _compute_balanced_caps(*, max_total: Optional[int], max_neg: Optional[int]) -> Tuple[Optional[int], Optional[int]]:
    """
    Compute desired (pos_count, neg_count) when `max_total` is set.

    We aim for a roughly balanced subset while respecting `max_neg` when provided.
    """
    if max_total is None:
        return None, None

    if max_total <= 0:
        return 0, 0

    target_neg = int(max_total // 2)
    if max_neg is not None:
        target_neg = min(target_neg, int(max_neg))
    if max_total > 1:
        target_neg = max(1, min(target_neg, max_total - 1))
    else:
        target_neg = 0
    target_pos = int(max_total - target_neg)
    return target_pos, target_neg


def _iter_windows_from_csv(
    csv_path: Path,
    *,
    window_points: int,
    target_user: str,
    target_width: int,
    keep_all_subjects: bool,
) -> Iterator[Tuple[np.ndarray, int]]:
    """
    Yield (window, label) where window is float32 shaped (1, 12, target_width).

    Window boundaries are determined by `window_id` (produced by the server pipeline).
    We also validate that all rows in a window share the same `subject` to avoid
    accidental cross-subject mixing when computing labels.
    """
    if not csv_path.exists():
        raise FileNotFoundError(f"Missing window CSV: {csv_path}")

    target_user_norm = str(target_user).strip()

    with csv_path.open("r", encoding="utf-8", newline="") as f:
        reader = csv.reader(f)
        try:
            raw_header = next(reader)
        except StopIteration:
            return

        header = [str(h).strip().lstrip("\ufeff") for h in raw_header]
        lookup = {name.lower(): i for i, name in enumerate(header)}
        try:
            idx_subject = lookup["subject"]
            idx_window_id = lookup["window_id"]
            idx_acc_x = lookup["acc_x"]
            idx_acc_y = lookup["acc_y"]
            idx_acc_z = lookup["acc_z"]
            idx_gyr_x = lookup["gyr_x"]
            idx_gyr_y = lookup["gyr_y"]
            idx_gyr_z = lookup["gyr_z"]
            idx_mag_x = lookup["mag_x"]
            idx_mag_y = lookup["mag_y"]
            idx_mag_z = lookup["mag_z"]
        except KeyError as exc:
            raise ValueError(f"Unexpected CSV header for {csv_path}: {header}") from exc

        current_window_id: Optional[str] = None
        current_subject: Optional[str] = None
        skip_window = False
        filled = 0
        window_raw = np.empty((12, window_points), dtype=np.float32)

        def _flush_current() -> Optional[Tuple[np.ndarray, int]]:
            nonlocal filled
            if current_window_id is None or current_subject is None:
                return None
            if skip_window:
                return None
            if filled != window_points:
                return None
            label = 1 if current_subject == target_user_norm else 0
            window = _resample_time_axis(window_raw, target_width)
            return window[np.newaxis, :, :], label

        while True:
            try:
                row = next(reader)
            except StopIteration:
                flushed = _flush_current()
                if flushed is not None:
                    yield flushed
                return

            if not row:
                continue

            row_window_id = str(row[idx_window_id]).strip()
            row_subject = str(row[idx_subject]).strip()

            if current_window_id is None:
                current_window_id = row_window_id
                current_subject = row_subject
                skip_window = (not keep_all_subjects) and (current_subject != target_user_norm)
                filled = 0
            elif row_window_id != current_window_id:
                flushed = _flush_current()
                if flushed is not None:
                    yield flushed
                current_window_id = row_window_id
                current_subject = row_subject
                skip_window = (not keep_all_subjects) and (current_subject != target_user_norm)
                filled = 0

            # If this window belongs to a non-target user and we only want positives,
            # we skip it without parsing float features.
            if skip_window:
                continue

            # Validate that a window does not mix subjects. If it does, discard it.
            if current_subject is None or row_subject != current_subject:
                skip_window = True
                continue

            if filled >= window_points:
                # Extra rows for the same window_id; treat as corrupted and skip.
                skip_window = True
                continue

            acc_x = float(row[idx_acc_x])
            acc_y = float(row[idx_acc_y])
            acc_z = float(row[idx_acc_z])
            gyr_x = float(row[idx_gyr_x])
            gyr_y = float(row[idx_gyr_y])
            gyr_z = float(row[idx_gyr_z])
            mag_x = float(row[idx_mag_x])
            mag_y = float(row[idx_mag_y])
            mag_z = float(row[idx_mag_z])

            window_raw[0, filled] = acc_x
            window_raw[1, filled] = acc_y
            window_raw[2, filled] = acc_z
            window_raw[3, filled] = float(np.sqrt(acc_x * acc_x + acc_y * acc_y + acc_z * acc_z))
            window_raw[4, filled] = gyr_x
            window_raw[5, filled] = gyr_y
            window_raw[6, filled] = gyr_z
            window_raw[7, filled] = float(np.sqrt(gyr_x * gyr_x + gyr_y * gyr_y + gyr_z * gyr_z))
            window_raw[8, filled] = mag_x
            window_raw[9, filled] = mag_y
            window_raw[10, filled] = mag_z
            window_raw[11, filled] = float(np.sqrt(mag_x * mag_x + mag_y * mag_y + mag_z * mag_z))
            filled += 1


def iter_windows_from_csv_unlabeled(
    csv_path: Path,
    *,
    window_size_sec: float,
    target_width: int,
) -> Iterator[Tuple[str, str, np.ndarray]]:
    """
    Iterate windows from a server-formatted CSV without assigning labels.

    Yields: (window_id, subject, window) where:
      - window is float32 shaped (1, 12, target_width)

    Notes:
      - Window boundaries are determined by `window_id`.
      - If a window mixes subjects or has extra/missing rows, it is skipped.
    """
    window_points = _window_points(window_size_sec)
    if not csv_path.exists():
        raise FileNotFoundError(f"Missing window CSV: {csv_path}")

    with csv_path.open("r", encoding="utf-8", newline="") as f:
        reader = csv.reader(f)
        try:
            raw_header = next(reader)
        except StopIteration:
            return

        header = [str(h).strip().lstrip("\ufeff") for h in raw_header]
        lookup = {name.lower(): i for i, name in enumerate(header)}
        try:
            idx_subject = lookup["subject"]
            idx_window_id = lookup["window_id"]
            idx_acc_x = lookup["acc_x"]
            idx_acc_y = lookup["acc_y"]
            idx_acc_z = lookup["acc_z"]
            idx_gyr_x = lookup["gyr_x"]
            idx_gyr_y = lookup["gyr_y"]
            idx_gyr_z = lookup["gyr_z"]
            idx_mag_x = lookup["mag_x"]
            idx_mag_y = lookup["mag_y"]
            idx_mag_z = lookup["mag_z"]
        except KeyError as exc:
            raise ValueError(f"Unexpected CSV header for {csv_path}: {header}") from exc

        current_window_id: Optional[str] = None
        current_subject: Optional[str] = None
        skip_window = False
        filled = 0
        window_raw = np.empty((12, window_points), dtype=np.float32)

        def _flush_current() -> Optional[Tuple[str, str, np.ndarray]]:
            nonlocal filled
            if current_window_id is None or current_subject is None:
                return None
            if skip_window:
                return None
            if filled != window_points:
                return None
            window = _resample_time_axis(window_raw, target_width)
            return str(current_window_id), str(current_subject), window[np.newaxis, :, :]

        while True:
            try:
                row = next(reader)
            except StopIteration:
                flushed = _flush_current()
                if flushed is not None:
                    yield flushed
                return

            if not row:
                continue

            row_window_id = str(row[idx_window_id]).strip()
            row_subject = str(row[idx_subject]).strip()

            if current_window_id is None:
                current_window_id = row_window_id
                current_subject = row_subject
                skip_window = False
                filled = 0
            elif row_window_id != current_window_id:
                flushed = _flush_current()
                if flushed is not None:
                    yield flushed
                current_window_id = row_window_id
                current_subject = row_subject
                skip_window = False
                filled = 0

            if skip_window:
                continue

            # Validate that a window does not mix subjects. If it does, discard it.
            if current_subject is None or row_subject != current_subject:
                skip_window = True
                continue

            if filled >= window_points:
                # Extra rows for the same window_id; treat as corrupted and skip.
                skip_window = True
                continue

            acc_x = float(row[idx_acc_x])
            acc_y = float(row[idx_acc_y])
            acc_z = float(row[idx_acc_z])
            gyr_x = float(row[idx_gyr_x])
            gyr_y = float(row[idx_gyr_y])
            gyr_z = float(row[idx_gyr_z])
            mag_x = float(row[idx_mag_x])
            mag_y = float(row[idx_mag_y])
            mag_z = float(row[idx_mag_z])

            window_raw[0, filled] = acc_x
            window_raw[1, filled] = acc_y
            window_raw[2, filled] = acc_z
            window_raw[3, filled] = float(np.sqrt(acc_x * acc_x + acc_y * acc_y + acc_z * acc_z))
            window_raw[4, filled] = gyr_x
            window_raw[5, filled] = gyr_y
            window_raw[6, filled] = gyr_z
            window_raw[7, filled] = float(np.sqrt(gyr_x * gyr_x + gyr_y * gyr_y + gyr_z * gyr_z))
            window_raw[8, filled] = mag_x
            window_raw[9, filled] = mag_y
            window_raw[10, filled] = mag_z
            window_raw[11, filled] = float(np.sqrt(mag_x * mag_x + mag_y * mag_y + mag_z * mag_z))
            filled += 1


def _load_split_windows(
    *,
    base_path: Path,
    user_id: str,
    window_size_sec: float,
    split: str,
    target_width: int,
    spec: _SplitLoadSpec,
) -> Tuple[np.ndarray, np.ndarray]:
    ws_dir = Path(base_path) / format_window_dir_name(window_size_sec) / user_id
    csv_path = ws_dir / f"{split}.csv"
    window_points = _window_points(window_size_sec)

    rng = np.random.default_rng(spec.seed)
    pos_windows: List[np.ndarray] = []
    neg_windows: List[np.ndarray] = []
    pos_seen = 0
    neg_seen = 0
    seen_negative = False

    # Determine caps for this split.
    #
    # - train: unbounded positives (genuine only) by default.
    # - val/test: keep memory bounded; if max_total is set, cap positives to the
    #   balanced target and allow negatives to fill the remainder.
    pos_cap = spec.max_pos
    neg_cap = spec.max_neg
    desired_pos: Optional[int] = None
    if split != "train":
        desired_pos, _ = _compute_balanced_caps(max_total=spec.max_total, max_neg=spec.max_neg)
        if pos_cap is None:
            # For eval, storing more positives than `desired_pos` does not help when
            # we later cap total windows; it only increases memory usage.
            if desired_pos is not None:
                pos_cap = desired_pos
            elif spec.max_neg is not None:
                # When no total cap is requested, bound positives to the negative cap.
                pos_cap = spec.max_neg

        if spec.max_total is not None:
            # We never need more than `max_total` negatives to fill the evaluation subset.
            if neg_cap is None:
                neg_cap = spec.max_total
            else:
                neg_cap = min(int(neg_cap), int(spec.max_total))

    keep_all_subjects = split != "train"
    iterator = _iter_windows_from_csv(
        csv_path,
        window_points=window_points,
        target_user=user_id,
        target_width=target_width,
        keep_all_subjects=keep_all_subjects,
    )

    for window, label in iterator:
        if label == 1:
            pos_seen += 1
            if pos_cap is None or len(pos_windows) < pos_cap:
                pos_windows.append(window)
            else:
                # Reservoir sampling: keep a uniform sample among windows scanned so far.
                j = int(rng.integers(0, pos_seen))
                if j < int(pos_cap):
                    pos_windows[j] = window
        else:
            neg_seen += 1
            seen_negative = True
            if neg_cap is None or len(neg_windows) < neg_cap:
                neg_windows.append(window)
            else:
                j = int(rng.integers(0, neg_seen))
                if j < int(neg_cap):
                    neg_windows[j] = window

        # Fast path: stop early for eval once we have enough negatives.
        # (Positives typically appear only in the prefix of val/test CSVs.)
        if (not spec.full_scan) and seen_negative:
            if spec.max_total is not None and (len(pos_windows) + len(neg_windows)) >= int(spec.max_total):
                break
            if neg_cap is not None and len(neg_windows) >= int(neg_cap):
                break

    # If a total cap is requested, trim negatives to fill the remainder after positives.
    if spec.max_total is not None:
        target_total = int(spec.max_total)
        if desired_pos is None:
            desired_pos = target_total // 2
        target_pos = min(len(pos_windows), int(desired_pos), target_total)
        target_neg = min(len(neg_windows), max(target_total - target_pos, 0))

        if len(pos_windows) > target_pos:
            idx = rng.choice(len(pos_windows), size=target_pos, replace=False)
            pos_windows = [pos_windows[i] for i in idx]
        if len(neg_windows) > target_neg:
            idx = rng.choice(len(neg_windows), size=target_neg, replace=False)
            neg_windows = [neg_windows[i] for i in idx]

    if not pos_windows and not neg_windows:
        empty_x = np.empty((0, 1, 12, target_width), dtype=np.float32)
        empty_y = np.empty((0,), dtype=np.int64)
        return empty_x, empty_y

    windows = np.stack(pos_windows + neg_windows, axis=0).astype(np.float32, copy=False)
    labels = np.array([1] * len(pos_windows) + [0] * len(neg_windows), dtype=np.int64)

    # Shuffle eval sets to avoid any ordering bias.
    if len(labels) > 1:
        perm = rng.permutation(len(labels))
        windows = windows[perm]
        labels = labels[perm]

    return windows, labels


def iter_windows_from_csv_unlabeled_with_session(
    csv_path: Path,
    *,
    window_size_sec: float,
    target_width: int,
) -> Iterator[Tuple[str, str, str, np.ndarray]]:
    """
    Iterate windows from a server-formatted CSV without assigning labels, keeping session metadata.

    Yields: (window_id, subject, session, window) where:
      - window is float32 shaped (1, 12, target_width)

    Notes:
      - Window boundaries are determined by `window_id`.
      - If a window mixes subjects/sessions or has extra/missing rows, it is skipped.
    """
    window_points = _window_points(window_size_sec)
    if not csv_path.exists():
        raise FileNotFoundError(f"Missing window CSV: {csv_path}")

    with csv_path.open("r", encoding="utf-8", newline="") as f:
        reader = csv.reader(f)
        try:
            raw_header = next(reader)
        except StopIteration:
            return

        header = [str(h).strip().lstrip("\ufeff") for h in raw_header]
        lookup = {name.lower(): i for i, name in enumerate(header)}
        try:
            idx_subject = lookup["subject"]
            idx_session = lookup["session"]
            idx_window_id = lookup["window_id"]
            idx_acc_x = lookup["acc_x"]
            idx_acc_y = lookup["acc_y"]
            idx_acc_z = lookup["acc_z"]
            idx_gyr_x = lookup["gyr_x"]
            idx_gyr_y = lookup["gyr_y"]
            idx_gyr_z = lookup["gyr_z"]
            idx_mag_x = lookup["mag_x"]
            idx_mag_y = lookup["mag_y"]
            idx_mag_z = lookup["mag_z"]
        except KeyError as exc:
            raise ValueError(f"Unexpected CSV header for {csv_path}: {header}") from exc

        current_window_id: Optional[str] = None
        current_subject: Optional[str] = None
        current_session: Optional[str] = None
        skip_window = False
        filled = 0
        window_raw = np.empty((12, window_points), dtype=np.float32)

        def _flush_current() -> Optional[Tuple[str, str, str, np.ndarray]]:
            nonlocal filled
            if current_window_id is None or current_subject is None or current_session is None:
                return None
            if skip_window:
                return None
            if filled != window_points:
                return None
            window = _resample_time_axis(window_raw, target_width)
            return str(current_window_id), str(current_subject), str(current_session), window[np.newaxis, :, :]

        while True:
            try:
                row = next(reader)
            except StopIteration:
                flushed = _flush_current()
                if flushed is not None:
                    yield flushed
                return

            if not row:
                continue

            row_window_id = str(row[idx_window_id]).strip()
            row_subject = str(row[idx_subject]).strip()
            row_session = str(row[idx_session]).strip()

            if current_window_id is None:
                current_window_id = row_window_id
                current_subject = row_subject
                current_session = row_session
                skip_window = False
                filled = 0
            elif row_window_id != current_window_id:
                flushed = _flush_current()
                if flushed is not None:
                    yield flushed
                current_window_id = row_window_id
                current_subject = row_subject
                current_session = row_session
                skip_window = False
                filled = 0

            if skip_window:
                continue

            # Validate that a window does not mix subjects/sessions. If it does, discard it.
            if current_subject is None or row_subject != current_subject:
                skip_window = True
                continue
            if current_session is None or row_session != current_session:
                skip_window = True
                continue

            if filled >= window_points:
                # Extra rows for the same window_id; treat as corrupted and skip.
                skip_window = True
                continue

            acc_x = float(row[idx_acc_x])
            acc_y = float(row[idx_acc_y])
            acc_z = float(row[idx_acc_z])
            gyr_x = float(row[idx_gyr_x])
            gyr_y = float(row[idx_gyr_y])
            gyr_z = float(row[idx_gyr_z])
            mag_x = float(row[idx_mag_x])
            mag_y = float(row[idx_mag_y])
            mag_z = float(row[idx_mag_z])

            window_raw[0, filled] = acc_x
            window_raw[1, filled] = acc_y
            window_raw[2, filled] = acc_z
            window_raw[3, filled] = float(np.sqrt(acc_x * acc_x + acc_y * acc_y + acc_z * acc_z))
            window_raw[4, filled] = gyr_x
            window_raw[5, filled] = gyr_y
            window_raw[6, filled] = gyr_z
            window_raw[7, filled] = float(np.sqrt(gyr_x * gyr_x + gyr_y * gyr_y + gyr_z * gyr_z))
            window_raw[8, filled] = mag_x
            window_raw[9, filled] = mag_y
            window_raw[10, filled] = mag_z
            window_raw[11, filled] = float(np.sqrt(mag_x * mag_x + mag_y * mag_y + mag_z * mag_z))
            filled += 1


def prepare_user_datasets(
    *,
    target_user: str,
    window_size_sec: float,
    cache=None,
    overlap: float = DEFAULT_OVERLAP,
    target_width: int = 50,
    prep_workers: int = 0,
    max_negative_per_split: int = 50_000,
    max_eval_per_split: Optional[int] = None,
    negative_users: Optional[Sequence[str]] = None,
    window_cache_dir: Optional[Path] = None,
    base_path: Optional[Path] = None,
    full_scan_eval: bool = False,
    seed: int = 42,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Load train/val/test window tensors for a given *target_user*.

    - train: only genuine windows (label=1)
    - val/test: genuine + attackers (label=1/0 based on subject == target_user)
    """
    del cache, overlap, prep_workers, negative_users, window_cache_dir  # kept for API compatibility
    if base_path is None:
        raise ValueError("base_path is required (should point to server processed_data/window)")

    base_path = Path(base_path)
    ws_tag = format_window_dir_name(window_size_sec)
    if not (base_path / ws_tag).exists():
        raise FileNotFoundError(f"Window directory not found: {base_path / ws_tag}")

    train_spec = _SplitLoadSpec(
        max_pos=None,
        max_neg=None,
        max_total=None,
        full_scan=False,
        seed=seed,
    )
    eval_spec = _SplitLoadSpec(
        max_pos=None,
        max_neg=max_negative_per_split,
        max_total=max_eval_per_split,
        full_scan=full_scan_eval,
        seed=seed,
    )

    train_x, train_y = _load_split_windows(
        base_path=base_path,
        user_id=target_user,
        window_size_sec=window_size_sec,
        split="train",
        target_width=target_width,
        spec=train_spec,
    )
    val_x, val_y = _load_split_windows(
        base_path=base_path,
        user_id=target_user,
        window_size_sec=window_size_sec,
        split="val",
        target_width=target_width,
        spec=eval_spec,
    )
    test_x, test_y = _load_split_windows(
        base_path=base_path,
        user_id=target_user,
        window_size_sec=window_size_sec,
        split="test",
        target_width=target_width,
        spec=eval_spec,
    )

    logger.info(
        "[DATA] user=%s ws=%s target_width=%s train=%d val(pos=%d,neg=%d) test(pos=%d,neg=%d)",
        target_user,
        ws_tag,
        target_width,
        len(train_y),
        int((val_y == 1).sum()),
        int((val_y == 0).sum()),
        int((test_y == 1).sum()),
        int((test_y == 0).sum()),
    )

    return train_x, train_y, val_x, val_y, test_x, test_y


def load_user_train_windows(
    *,
    target_user: str,
    window_size_sec: float,
    target_width: int = 50,
    base_path: Path,
    seed: int = 42,
    max_train_windows: Optional[int] = None,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Load **only** the train split windows (genuine-only) for a given target_user.

    This is used by full-stream evaluation modes where val/test are scored directly
    from CSV to avoid materializing huge window tensors in memory.
    """
    base_path = Path(base_path)
    ws_tag = format_window_dir_name(window_size_sec)
    ws_dir = base_path / ws_tag / str(target_user)
    if not ws_dir.exists():
        raise FileNotFoundError(f"User directory not found: {ws_dir}")

    train_spec = _SplitLoadSpec(
        max_pos=None,
        max_neg=None,
        max_total=None,
        full_scan=False,
        seed=int(seed),
    )
    train_x, train_y = _load_split_windows(
        base_path=base_path,
        user_id=str(target_user),
        window_size_sec=float(window_size_sec),
        split="train",
        target_width=int(target_width),
        spec=train_spec,
    )

    if max_train_windows is not None and int(max_train_windows) > 0 and int(train_x.shape[0]) > int(max_train_windows):
        rng = np.random.default_rng(int(seed))
        idx = rng.choice(int(train_x.shape[0]), size=int(max_train_windows), replace=False)
        train_x = train_x[idx]
        train_y = train_y[idx]

    return train_x, train_y


def precompute_all_user_windows(
    *,
    base_path: Path,
    cache_dir: Path,
    users: Sequence[str],
    window_sizes: Sequence[float],
    overlap: float = DEFAULT_OVERLAP,
    target_width: int = 50,
    prep_workers: int = 0,
    process_workers: int = 0,
) -> List[Tuple[str, float]]:
    """
    Compatibility hook for hmog_vq_experiment.py.

    Server datasets are already windowed, so we just verify the expected CSV files exist.
    """
    del cache_dir, overlap, target_width, prep_workers, process_workers
    ready: List[Tuple[str, float]] = []
    base_path = Path(base_path)
    for user in users:
        for ws in window_sizes:
            ws_dir = base_path / format_window_dir_name(ws) / user
            ok = all((ws_dir / f"{split}.csv").exists() for split in ("train", "val", "test"))
            if ok:
                ready.append((user, ws))
            else:
                missing = [split for split in ("train", "val", "test") if not (ws_dir / f"{split}.csv").exists()]
                logger.warning("[DATA] Missing %s for user=%s ws=%s under %s", missing, user, ws, ws_dir)
    return ready
